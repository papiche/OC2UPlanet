from .base_agent import Agent
import json
import os
import subprocess
import random
from collections import defaultdict, Counter

class AnalystAgent(Agent):
    """
    L'agent Analyste utilise l'IA pour analyser la base de prospects,
    identifier des clusters socio-g√©ographiques, et permettre √† l'utilisateur
    de choisir une cible strat√©gique. Il propose deux modes : Rapide et Profond.
    """

    def _clean_ia_json_output(self, ia_output_str: str) -> str:
        """
        Nettoie la sortie brute de l'IA pour en extraire une cha√Æne JSON valide.
        Supprime les blocs de code Markdown (```json...```) et autres textes parasites.
        """
        cleaned_str = ia_output_str.strip()
        
        # Trouve le d√©but du JSON (soit {, soit [)
        json_start = -1
        for i, char in enumerate(cleaned_str):
            if char in ['{', '[']:
                json_start = i
                break
        
        if json_start == -1:
            return cleaned_str # Pas de JSON trouv√©

        # Trouve la fin du JSON (soit }, soit ])
        json_end = -1
        # On parcourt la cha√Æne √† l'envers
        for i, char in enumerate(reversed(cleaned_str)):
            if char in ['}', ']']:
                json_end = len(cleaned_str) - 1 - i
                break
        
        if json_end == -1:
            return cleaned_str

        return cleaned_str[json_start:json_end+1]

    def _load_and_sync_knowledge_base(self):
        """
        Charge la base de connaissance existante, la synchronise avec le
        fichier de prospects source pour ajouter/mettre √† jour les entr√©es,
        et la retourne. La base est un dictionnaire index√© par pubkey.
        """
        kb_file = self.shared_state['config']['enriched_prospects_file']
        prospect_file = os.path.expanduser(self.shared_state['config']['prospect_file'])
        
        # 1. Charger la base de connaissance existante
        knowledge_base = {}
        if os.path.exists(kb_file):
            try:
                with open(kb_file, 'r') as f:
                    knowledge_base = json.load(f)
                self.logger.info(f"{len(knowledge_base)} profils charg√©s depuis la base de connaissance.")
            except (json.JSONDecodeError, IOError) as e:
                self.logger.error(f"Impossible de charger la base de connaissance '{kb_file}'. Erreur : {e}")

        # 2. Lire le fichier source et synchroniser
        if not os.path.exists(prospect_file):
            self.logger.error(f"Fichier de prospects source '{prospect_file}' non trouv√©.")
            return knowledge_base # On retourne ce qu'on a

        try:
            with open(prospect_file, 'r') as f:
                source_data = json.load(f)

            source_prospects_count = 0
            new_prospects_count = 0
            for member in source_data.get('members', []):
                pubkey = member.get("pubkey")
                if not pubkey: continue
                source_prospects_count += 1

                if pubkey not in knowledge_base:
                    knowledge_base[pubkey] = {
                        "uid": member.get("uid"),
                        "profile": member.get('profile', {}),
                        "source": member.get('source'),
                        "metadata": {} 
                    }
                    new_prospects_count += 1
                else: 
                    knowledge_base[pubkey]['profile'] = member.get('profile', {})

            self.logger.info(f"Synchronisation termin√©e : {source_prospects_count} profils dans la source, {new_prospects_count} nouveaux ajout√©s.")
        except Exception as e:
            self.logger.error(f"Erreur lors de la synchronisation avec '{prospect_file}': {e}", exc_info=True)
            
        return knowledge_base

    def _save_knowledge_base(self, knowledge_base):
        """Sauvegarde la base de connaissance enrichie dans son fichier."""
        kb_file = self.shared_state['config']['enriched_prospects_file']
        try:
            with open(kb_file, 'w') as f:
                json.dump(knowledge_base, f, indent=2, ensure_ascii=False)
            self.logger.info(f"Base de connaissance sauvegard√©e avec {len(knowledge_base)} profils.")
        except IOError as e:
            self.logger.error(f"Impossible de sauvegarder la base de connaissance : {e}")

    def get_analysis_progress(self):
        """
        Calcule et retourne l'√©tat d'avancement de l'enrichissement
        de la base de connaissance.
        """
        knowledge_base = self._load_and_sync_knowledge_base()
        total_prospects = len(knowledge_base)
        
        language_analyzed = 0
        tags_analyzed = 0
        
        for pk, data in knowledge_base.items():
            metadata = data.get('metadata', {})
            if 'language' in metadata:
                language_analyzed += 1
            if 'tags' in metadata:
                tags_analyzed += 1
                
        return {
            "total": total_prospects,
            "language": language_analyzed,
            "tags": tags_analyzed
        }

    def run_geo_linguistic_analysis(self):
        """
        Analyse les descriptions pour en extraire la langue, le pays et la r√©gion
        et sauvegarde ces donn√©es dans la base de connaissance.
        """
        self.logger.info("ü§ñ Agent Analyste : D√©marrage de l'analyse G√©o-Linguistique (avec persistance)...")
        self.shared_state['status']['AnalystAgent'] = "Analyse G√©o-Linguistique en cours..."

        if not self._check_ollama_once():
            self.shared_state['status']['AnalystAgent'] = "√âchec : API Ollama indisponible."
            return
            
        knowledge_base = self._load_and_sync_knowledge_base()
        prospects_to_analyze = [pk for pk, data in knowledge_base.items() if 'g1_wot' in data.get('source', '')]
        
        geo_prompt_template = self._load_prompt('analyst_language_prompt_file')
        if not geo_prompt_template: return

        needs_analysis_count = 0
        save_interval = 50
        
        for i, pubkey in enumerate(prospects_to_analyze):
            prospect_data = knowledge_base[pubkey]
            
            if 'language' in prospect_data.get('metadata', {}):
                continue
            
            needs_analysis_count += 1
            profile = prospect_data.get('profile', {})
            description = (profile.get('_source', {}).get('description') or '').strip()

            if not description:
                meta = prospect_data.setdefault('metadata', {})
                meta['language'] = 'xx'
                meta['country'] = None
                meta['region'] = None
                continue

            self.logger.info(f"Analyse G√©o-Linguistique {needs_analysis_count}/{len(prospects_to_analyze)} : {prospect_data.get('uid', 'N/A')}")
            prompt = f"{geo_prompt_template}\n\nTexte fourni: \"{description}\""
            
            try:
                ia_response = self._query_ia(prompt, expect_json=True)
                cleaned_answer = self._clean_ia_json_output(ia_response['answer'])
                geo_data = json.loads(cleaned_answer)
                
                meta = prospect_data.setdefault('metadata', {})
                meta['language'] = geo_data.get('language', 'xx')
                meta['country'] = geo_data.get('country')
                meta['region'] = geo_data.get('region')

                if needs_analysis_count > 0 and needs_analysis_count % save_interval == 0:
                    self.logger.info(f"--- Sauvegarde interm√©diaire de la base de connaissance ({needs_analysis_count} profils analys√©s)... ---")
                    self._save_knowledge_base(knowledge_base)
            except Exception as e:
                self.logger.error(f"Impossible de g√©o-classifier le profil {prospect_data.get('uid')} : {e}")

        self.logger.info(f"Analyse G√©o-Linguistique termin√©e. {needs_analysis_count} nouveaux profils ont √©t√© analys√©s. Sauvegarde finale.")
        self._save_knowledge_base(knowledge_base)

        # Agr√©ger les r√©sultats par PAYS
        self.logger.info("--- Agr√©gation des r√©sultats par Pays ---")
        members_by_country = defaultdict(list)
        for pubkey, data in knowledge_base.items():
            country = data.get('metadata', {}).get('country')
            if country:
                members_by_country[country].append(data)
        
        clusters = []
        sorted_countries = sorted(members_by_country.items(), key=lambda item: len(item[1]), reverse=True)

        for country, members in sorted_countries:
            clusters.append({
                "cluster_name": f"Pays : {country}",
                "description": f"Groupe de {len(members)} membres localis√©s en '{country}'.",
                "members": members
            })
        
        self._select_and_save_cluster(clusters)

    def select_cluster_from_tags(self):
        """
        Agr√®ge les tags existants dans la base de connaissance,
        pr√©sente les th√®mes sous forme de clusters, et permet √† 
        l'utilisateur de s√©lectionner une cible pour une campagne.
        """
        self.logger.info("ü§ñ Agent Analyste : Pr√©paration du ciblage par th√®me...")
        knowledge_base = self._load_and_sync_knowledge_base()
        
        # Agr√©ger les r√©sultats
        self.logger.info("--- Agr√©gation des th√®mes existants ---")
        members_by_tag = defaultdict(list)
        for pubkey, data in knowledge_base.items():
            if 'tags' in data.get('metadata', {}):
                for tag in data['metadata']['tags']:
                    members_by_tag[tag].append(data)
        
        if not members_by_tag:
            self.logger.warning("Aucun th√®me n'a encore √©t√© analys√©. Veuillez lancer l'analyse par th√®mes d'abord (option 2).")
            self.shared_state['status']['AnalystAgent'] = "Aucun th√®me √† cibler."
            return

        clusters = []
        # Trier par nombre de membres, du plus grand au plus petit
        sorted_tags = sorted(members_by_tag.items(), key=lambda item: len(item[1]), reverse=True)

        # Limiter √† l'affichage des 50 th√®mes les plus populaires pour ne pas surcharger le menu
        for tag, members in sorted_tags[:50]:
            clusters.append({
                "cluster_name": f"Th√®me : {tag.capitalize()}",
                "description": f"Groupe de {len(members)} membres partageant l'int√©r√™t ou la comp√©tence '{tag}'.",
                "members": members
            })
        
        self.logger.info("Les 50 th√®mes les plus populaires :")
        self._select_and_save_cluster(clusters)

    def run_thematic_analysis(self):
        """
        Analyse les descriptions des prospects pour en extraire des mots-cl√©s
        th√©matiques (tags) et les sauvegarde dans la base de connaissance.
        """
        self.logger.info("ü§ñ Agent Analyste : D√©marrage de l'analyse th√©matique (avec persistance)...")
        self.shared_state['status']['AnalystAgent'] = "Analyse th√©matique en cours..."

        if not self._check_ollama_once():
            self.shared_state['status']['AnalystAgent'] = "√âchec : API Ollama indisponible."
            return
            
        knowledge_base = self._load_and_sync_knowledge_base()
        
        # --- Calculer les th√®mes les plus pertinents pour guider l'IA ---
        tag_counter = Counter()
        for pk, data in knowledge_base.items():
            if 'tags' in data.get('metadata', {}):
                tag_counter.update(data['metadata']['tags'])
        
        # On prend les 100 th√®mes les plus fr√©quents comme guide
        guide_tags = [tag for tag, count in tag_counter.most_common(100)]
        if guide_tags:
            self.logger.info(f"Utilisation des {len(guide_tags)} th√®mes les plus fr√©quents comme guide pour l'IA.")

        prospects_to_analyze = [pk for pk, data in knowledge_base.items() if 'g1_wot' in data.get('source', '')]
        
        thematic_prompt_template = self._load_prompt('analyst_thematic_prompt_file')
        if not thematic_prompt_template: return

        needs_analysis_count = 0
        save_interval = 50
        
        for i, pubkey in enumerate(prospects_to_analyze):
            prospect_data = knowledge_base[pubkey]
            
            metadata = prospect_data.setdefault('metadata', {})
            if 'tags' in metadata: # 'tags' peut √™tre une liste vide ou ['error']
                continue
            
            needs_analysis_count += 1
            profile = prospect_data.get('profile', {})
            description = (profile.get('_source', {}).get('description') or '').strip()

            if not description:
                metadata['tags'] = []
                continue

            self.logger.info(f"Analyse th√©matique {needs_analysis_count}/{len(prospects_to_analyze)} : {prospect_data.get('uid', 'N/A')}")
            
            # Construire le prompt guid√© avec la liste concise
            prompt = f"{thematic_prompt_template}\n\nTexte fourni: \"{description}\""
            if guide_tags:
                prompt += f"\nTh√®mes existants : {json.dumps(guide_tags)}"
            
            try:
                ia_response = self._query_ia(prompt, expect_json=True)
                cleaned_answer = self._clean_ia_json_output(ia_response['answer'])
                tags = json.loads(cleaned_answer)

                # --- VALIDATION STRICTE ---
                if not isinstance(tags, list) or len(tags) > 7:
                    self.logger.warning(f"R√©ponse IA invalide pour {prospect_data.get('uid')} (format ou trop de tags). Marqu√© comme erreur.")
                    metadata['tags'] = ['error']
                    continue
                
                metadata['tags'] = tags
                
                if needs_analysis_count > 0 and needs_analysis_count % save_interval == 0:
                    self.logger.info(f"--- Sauvegarde interm√©diaire de la base de connaissance ({needs_analysis_count} profils analys√©s)... ---")
                    self._save_knowledge_base(knowledge_base)
            except Exception as e:
                self.logger.error(f"Impossible de tagger le profil {prospect_data.get('uid')} : {e}")
                metadata['tags'] = ['error']

        self.logger.info(f"Analyse th√©matique termin√©e. {needs_analysis_count} nouveaux profils ont √©t√© tagg√©s. Sauvegarde finale.")
        self._save_knowledge_base(knowledge_base)

        # Agr√©ger les r√©sultats
        self.logger.info("--- Agr√©gation des r√©sultats th√©matiques ---")
        members_by_tag = defaultdict(list)
        for pubkey, data in knowledge_base.items():
            if 'tags' in data.get('metadata', {}):
                for tag in data['metadata']['tags']:
                    members_by_tag[tag].append(data)
        
        clusters = []
        sorted_tags = sorted(members_by_tag.items(), key=lambda item: len(item[1]), reverse=True)

        for tag, members in sorted_tags:
            clusters.append({
                "cluster_name": f"Th√®me : {tag.capitalize()}",
                "description": f"Groupe de {len(members)} membres partageant l'int√©r√™t ou la comp√©tence '{tag}'.",
                "members": members
            })
        
        self._select_and_save_cluster(clusters)

    def run_test_mode(self):
        """
        G√©n√®re un fichier de cible avec un unique profil de test
        pour valider rapidement les agents Strat√®ge et Op√©rateur.
        """
        self.logger.info("ü§ñ Agent Analyste : Activation du Mode Test...")
        self.shared_state['status']['AnalystAgent'] = "Mode Test activ√©."
        
        test_pubkey = "DsEx1pS33vzYZg4MroyBV9hCw98j1gtHEhwiZ5tK7ech"

        # On charge la base pour avoir le profil complet
        knowledge_base = self._load_and_sync_knowledge_base()
        
        target_profile = knowledge_base.get(test_pubkey)

        if not target_profile:
            self.logger.error(f"Le profil de test avec la cl√© publique '{test_pubkey}' n'a pas √©t√© trouv√© dans la base de connaissance.")
            self.shared_state['status']['AnalystAgent'] = "√âchec : Profil de test non trouv√©."
            return

        # On ne met que les informations n√©cessaires pour les agents suivants
        final_targets = [{
            "pubkey": test_pubkey,
            "uid": target_profile.get("uid"),
            "profile": target_profile.get("profile") # Le strat√®ge peut en avoir besoin
        }]

        # On utilise la m√™me m√©thode de sauvegarde que pour les vrais clusters
        target_file = os.path.join(self.shared_state['config']['workspace'], "todays_targets.json")
        try:
            with open(target_file, 'w') as f:
                json.dump(final_targets, f, indent=4, ensure_ascii=False)

            report = f"Mode Test : Cible unique '{target_profile.get('uid')}' enregistr√©e."
            self.logger.info(f"‚úÖ {report}")
            self.shared_state['status']['AnalystAgent'] = report
            self.shared_state['analyst_report'] = "Cible du jour : Profil de test unique."
            self.shared_state['targets'] = final_targets
        except (IOError, TypeError) as e:
            self.logger.error(f"Impossible de cr√©er le fichier de cible de test : {e}")
            self.shared_state['status']['AnalystAgent'] = "√âchec de la cr√©ation du fichier de test." 

    def create_automatic_personas(self):
        """
        Cr√©e automatiquement des personas bas√©s sur les th√®mes les plus fr√©quents
        d√©tect√©s dans la base de connaissance et remplit les banques 5-9.
        """
        self.logger.info("üé≠ Agent Analyste : Cr√©ation automatique de personas bas√©s sur les th√®mes...")
        
        # V√©rifier Ollama
        if not self._check_ollama_once():
            self.logger.error("‚ùå Ollama non disponible. Impossible de cr√©er les personas.")
            return
        
        # Charger la base de connaissance
        knowledge_base = self._load_and_sync_knowledge_base()
        if not knowledge_base:
            self.logger.error("‚ùå Base de connaissance vide. Impossible de cr√©er les personas.")
            return
        
        # Analyser les th√®mes existants
        all_tags = []
        analyzed_profiles = 0
        for pubkey, data in knowledge_base.items():
            metadata = data.get('metadata', {})
            tags = metadata.get('tags', [])
            if tags and tags != ['error']:
                all_tags.extend(tags)
                analyzed_profiles += 1
        
        total_profiles = len(knowledge_base)
        self.logger.info(f"üìä Profils analys√©s : {analyzed_profiles} / {total_profiles}")
        
        # V√©rifier si l'analyse est suffisante
        if not all_tags:
            self.logger.error("‚ùå Aucun th√®me d√©tect√© dans la base de connaissance. Lancez d'abord l'analyse th√©matique.")
            return
        
        # Si moins de 10% des profils sont analys√©s, proposer de lancer l'analyse
        if analyzed_profiles < total_profiles * 0.1:
            self.logger.warning(f"‚ö†Ô∏è Seulement {analyzed_profiles} profils analys√©s sur {total_profiles} ({analyzed_profiles/total_profiles*100:.1f}%)")
            self.logger.warning("‚ö†Ô∏è L'analyse th√©matique semble incompl√®te. Les personas g√©n√©r√©s pourraient ne pas √™tre repr√©sentatifs.")
            
            choice = input("Voulez-vous lancer l'analyse th√©matique compl√®te maintenant ? (o/n) : ").strip().lower()
            if choice in ['o', 'oui', 'y', 'yes']:
                self.logger.info("üîÑ Lancement de l'analyse th√©matique compl√®te...")
                self.run_thematic_analysis()
                
                # Recharger les donn√©es apr√®s analyse
                knowledge_base = self._load_and_sync_knowledge_base()
                all_tags = []
                analyzed_profiles = 0
                for pubkey, data in knowledge_base.items():
                    metadata = data.get('metadata', {})
                    tags = metadata.get('tags', [])
                    if tags and tags != ['error']:
                        all_tags.extend(tags)
                        analyzed_profiles += 1
                
                self.logger.info(f"üìä Profils analys√©s apr√®s analyse compl√®te : {analyzed_profiles} / {total_profiles}")
            else:
                self.logger.info("‚ÑπÔ∏è Continuation avec les donn√©es existantes...")
        
        # Compter les occurrences et prendre le top 5
        tag_counts = Counter(all_tags)
        top_5_themes = tag_counts.most_common(5)
        
        self.logger.info(f"üìä Top 5 des th√®mes d√©tect√©s :")
        for i, (theme, count) in enumerate(top_5_themes, 1):
            self.logger.info(f"  {i}. {theme} ({count} occurrences)")
        
        # Charger la configuration des banques existante
        banks_config_file = os.path.join(self.shared_state['config']['workspace'], 'memory_banks_config.json')
        banks_config = self._load_banks_config(banks_config_file)
        
        # Cr√©er les personas pour les banques 5-9
        for i, (theme, count) in enumerate(top_5_themes):
            bank_slot = str(5 + i)  # Banques 5, 6, 7, 8, 9
            
            self.logger.info(f"üé≠ Cr√©ation du persona pour le th√®me '{theme}' (banque {bank_slot})...")
            
            # G√©n√©rer le persona avec l'IA
            persona = self._generate_persona_for_theme(theme, count, all_tags)
            
            if persona:
                # Remplir la banque
                banks_config['banks'][bank_slot] = {
                    'name': persona['name'],
                    'archetype': persona['archetype'],
                    'description': persona['description'],
                    'themes': [theme],
                    'corpus': persona['corpus']
                }
                
                self.logger.info(f"‚úÖ Persona cr√©√© : {persona['name']} ({persona['archetype']})")
            else:
                self.logger.warning(f"‚ö†Ô∏è √âchec de cr√©ation du persona pour le th√®me '{theme}'")
        
        # Sauvegarder la configuration mise √† jour
        self._save_banks_config(banks_config, banks_config_file)
        
        self.logger.info(f"üéâ Cr√©ation automatique termin√©e ! {len(top_5_themes)} personas cr√©√©s dans les banques 5-9.")
        
        # Afficher un r√©sum√©
        self.logger.info(f"\nüìã R√âSUM√â DES PERSONAS CR√â√âS :")
        for i, (theme, count) in enumerate(top_5_themes):
            bank_slot = str(5 + i)
            bank = banks_config['banks'].get(bank_slot, {})
            if bank:
                self.logger.info(f"  Banque {bank_slot} : {bank['name']} ({bank['archetype']}) - Th√®me : {theme}")

    def _generate_persona_for_theme(self, theme, theme_count, all_tags):
        """
        G√©n√®re un persona complet pour un th√®me donn√© en utilisant l'IA.
        """
        # Construire le contexte avec les th√®mes associ√©s
        related_themes = [t for t in all_tags if t != theme and t in all_tags]
        related_themes_sample = related_themes[:10]  # Limiter pour √©viter un prompt trop long
        
        prompt = f"""Tu es un expert en cr√©ation de personas marketing. Tu dois cr√©er un persona complet pour une campagne de communication UPlanet.

TH√àME PRINCIPAL : {theme}
OCCURRENCES D√âTECT√âES : {theme_count}
TH√àMES ASSOCI√âS : {', '.join(related_themes_sample)}

T√ÇCHE : Cr√©er un persona marketing complet avec :
1. Un nom accrocheur
2. Un arch√©type psychologique
3. Une description du profil type
4. Un corpus de communication (vocabulaire, arguments, ton, exemples)

Le persona doit √™tre adapt√© pour communiquer avec des personnes int√©ress√©es par le th√®me "{theme}" dans le contexte d'UPlanet (monnaie libre, identit√© num√©rique, d√©centralisation).

R√âPONSE ATTENDUE (JSON strict) :
{{
    "name": "Nom du Persona",
    "archetype": "Arch√©type psychologique",
    "description": "Description d√©taill√©e du profil type",
    "corpus": {{
        "tone": "Ton de communication (ex: bienveillant, technique, militant)",
        "vocabulary": ["mot1", "mot2", "mot3", "mot4", "mot5"],
        "arguments": [
            "Argument principal 1",
            "Argument principal 2", 
            "Argument principal 3"
        ],
        "examples": [
            "Exemple de phrase 1",
            "Exemple de phrase 2",
            "Exemple de phrase 3"
        ]
    }}
}}

IMPORTANT : 
- Le vocabulaire doit √™tre sp√©cifique au th√®me {theme}
- Les arguments doivent expliquer pourquoi UPlanet int√©resse ce profil
- Le ton doit √™tre adapt√© √† l'arch√©type
- Les exemples doivent √™tre des phrases compl√®tes et engageantes
- R√©ponds UNIQUEMENT en JSON valide, sans commentaire."""

        try:
            response = self._query_ia(prompt, expect_json=True)
            cleaned_response = self._clean_ia_json_output(response['answer'])
            persona = json.loads(cleaned_response)
            
            # Validation de la structure
            required_keys = ['name', 'archetype', 'description', 'corpus']
            corpus_keys = ['tone', 'vocabulary', 'arguments', 'examples']
            
            if not all(key in persona for key in required_keys):
                self.logger.error(f"Structure du persona invalide pour {theme}")
                return None
                
            if not all(key in persona['corpus'] for key in corpus_keys):
                self.logger.error(f"Structure du corpus invalide pour {theme}")
                return None
            
            return persona
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la g√©n√©ration du persona pour {theme} : {e}")
            return None

    def _load_banks_config(self, config_file):
        """Charge la configuration des banques de m√©moire."""
        if os.path.exists(config_file):
            try:
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                self.logger.error(f"Erreur lors du chargement de la config des banques : {e}")
        
        # Configuration par d√©faut si le fichier n'existe pas
        return {
            'banks': {},
            'available_themes': []
        }

    def _save_banks_config(self, banks_config, config_file):
        """Sauvegarde la configuration des banques de m√©moire."""
        try:
            os.makedirs(os.path.dirname(config_file), exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(banks_config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            self.logger.error(f"Erreur lors de la sauvegarde de la config des banques : {e}")

    def _prepare_data(self):
        prospect_file = os.path.expanduser(self.shared_state['config']['prospect_file'])
        blocklist_file = self.shared_state['config']['blocklist_file']
        
        blocklist = set()
        if os.path.exists(blocklist_file):
            try:
                with open(blocklist_file, 'r') as f:
                    blocklist = set(u.get('pubkey') for u in json.load(f))
            except (json.JSONDecodeError, IOError):
                self.logger.warning(f"Impossible de lire la blocklist '{blocklist_file}'.")

        if not os.path.exists(prospect_file):
            self.logger.error(f"Fichier de prospects '{prospect_file}' non trouv√©.")
            return []
        
        qualified_prospects = []
        try:
            with open(prospect_file, 'r') as f:
                data = json.load(f)
            
            for member in data.get('members', []):
                pubkey = member.get("pubkey")
                if pubkey in blocklist: continue
                if 'g1_wot' in member.get('source', ''):
                    profile = member.get('profile', {}).get('_source', {})
                    qualified_prospects.append({
                        "uid": member.get("uid"),
                        "pubkey": pubkey,
                        "description": profile.get("description"),
                        "city": profile.get("city"),
                        "geoPoint": profile.get("geoPoint")
                    })
            self.logger.info(f"{len(qualified_prospects)} prospects qualifi√©s ('B√¢tisseurs') trouv√©s pour l'analyse.")
            return qualified_prospects
        except Exception as e:
            self.logger.error(f"Erreur lors de la pr√©paration des donn√©es : {e}", exc_info=True)
            return []

    def _query_ia(self, prompt, expect_json=False):
        command = ['python3', self.shared_state['config']['question_script'], prompt]
        if expect_json:
            command.append('--json')
        
        self.logger.debug(f"Ex√©cution de la commande IA : {' '.join(command[:2])}...")
        result = subprocess.run(command, capture_output=True, text=True, check=True)

        self.logger.debug(f"R√©ponse brute de l'IA re√ßue : {result.stdout.strip()}")

        if expect_json:
            return json.loads(result.stdout)
        return result.stdout

    def _load_prompt(self, prompt_key):
        try:
            prompt_file = self.shared_state['config'][prompt_key]
            with open(prompt_file, 'r') as f:
                return f.read()
        except (KeyError, FileNotFoundError) as e:
            self.logger.error(f"Impossible de charger le fichier de prompt '{prompt_key}': {e}")
            return None

    def _select_and_save_cluster(self, clusters):
        if not clusters or not isinstance(clusters, list):
            self.logger.warning("Aucun cluster valide n'a √©t√© retourn√© par l'IA.")
            self.shared_state['status']['AnalystAgent'] = "Termin√© : Aucun cluster form√©."
            return

        self.logger.info("Clusters finaux identifi√©s par l'IA :")
        
        valid_clusters = []
        for i, cluster in enumerate(clusters):
            # --- VALIDATION DE LA STRUCTURE DU CLUSTER ---
            if not isinstance(cluster, dict):
                self.logger.warning(f"Item {i} n'est pas un cluster valide (n'est pas un dictionnaire), ignor√©.")
                continue

            name = cluster.get('cluster_name', f'Cluster sans nom {i+1}')
            desc = cluster.get('description', 'Pas de description.')
            members = cluster.get('members')

            if not isinstance(members, list):
                self.logger.warning(f"Le cluster '{name}' a √©t√© ignor√© car sa liste de membres est invalide ou manquante.")
                continue
            
            valid_clusters.append(cluster)
            
            print(f"\n{len(valid_clusters)}. {name} ({len(members)} membres)")
            print(f"   Description : {desc}")
        
        if not valid_clusters:
            self.logger.error("Aucun des clusters retourn√©s par l'IA n'avait le format requis. Impossible de continuer.")
            self.shared_state['status']['AnalystAgent'] = "√âchec : Format de cluster invalide."
            return

        try:
            choice = int(input("\nChoisissez le num√©ro du cluster √† cibler : ")) - 1
            if not (0 <= choice < len(valid_clusters)): raise ValueError()
            
            selected_cluster = valid_clusters[choice]
            final_targets = selected_cluster['members']

            target_file = os.path.join(self.shared_state['config']['workspace'], "todays_targets.json")
            with open(target_file, 'w') as f:
                json.dump(final_targets, f, indent=4, ensure_ascii=False)

            report = f"Cluster '{selected_cluster['cluster_name']}' s√©lectionn√©. {len(final_targets)} cibles enregistr√©es."
            self.logger.info(f"‚úÖ {report}")
            self.shared_state['status']['AnalystAgent'] = report
            self.shared_state['analyst_report'] = f"Cible du jour : {selected_cluster['description']}"
            self.shared_state['targets'] = final_targets
        except (ValueError, IndexError):
            self.logger.error("Choix invalide.")
        except KeyboardInterrupt:
            self.logger.info("Op√©ration annul√©e.") 

    def _check_ollama_once(self):
        """V√©rifie une seule fois que l'API Ollama est disponible."""
        if not getattr(self, '_ollama_checked', False):
            ollama_script = self.shared_state['config']['ollama_script']
            self.logger.info("V√©rification de la disponibilit√© de l'API Ollama...")
            self.logger.debug(f"Ex√©cution du script de v√©rification : {ollama_script}")
            try:
                subprocess.run([ollama_script], check=True, capture_output=True, text=True)
                self.logger.info("‚úÖ API Ollama accessible.")
                self._ollama_checked = True
            except (subprocess.CalledProcessError, FileNotFoundError) as e:
                self.logger.error(f"L'API Ollama n'est pas accessible. Erreur : {e}")
                self._ollama_checked = False
        return self._ollama_checked 

    def run_test_mode(self):
        """
        G√©n√®re un fichier de cible avec un unique profil de test
        pour valider rapidement les agents Strat√®ge et Op√©rateur.
        """
        self.logger.info("ü§ñ Agent Analyste : Activation du Mode Test...")
        self.shared_state['status']['AnalystAgent'] = "Mode Test activ√©."
        
        test_pubkey = "DsEx1pS33vzYZg4MroyBV9hCw98j1gtHEhwiZ5tK7ech"

        # On charge la base pour avoir le profil complet
        knowledge_base = self._load_and_sync_knowledge_base()
        
        target_profile = knowledge_base.get(test_pubkey)

        if not target_profile:
            self.logger.error(f"Le profil de test avec la cl√© publique '{test_pubkey}' n'a pas √©t√© trouv√© dans la base de connaissance.")
            self.shared_state['status']['AnalystAgent'] = "√âchec : Profil de test non trouv√©."
            return

        # On ne met que les informations n√©cessaires pour les agents suivants
        final_targets = [{
            "pubkey": test_pubkey,
            "uid": target_profile.get("uid"),
            "profile": target_profile.get("profile") # Le strat√®ge peut en avoir besoin
        }]

        # On utilise la m√™me m√©thode de sauvegarde que pour les vrais clusters
        target_file = os.path.join(self.shared_state['config']['workspace'], "todays_targets.json")
        try:
            with open(target_file, 'w') as f:
                json.dump(final_targets, f, indent=4, ensure_ascii=False)

            report = f"Mode Test : Cible unique '{target_profile.get('uid')}' enregistr√©e."
            self.logger.info(f"‚úÖ {report}")
            self.shared_state['status']['AnalystAgent'] = report
            self.shared_state['analyst_report'] = "Cible du jour : Profil de test unique."
            self.shared_state['targets'] = final_targets
        except (IOError, TypeError) as e:
            self.logger.error(f"Impossible de cr√©er le fichier de cible de test : {e}")
            self.shared_state['status']['AnalystAgent'] = "√âchec de la cr√©ation du fichier de test." 